{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b636bab-7b6e-498c-b00b-c863f1d9660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from unsloth import FastLanguageModel\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6836b0fc-c5d8-4683-9a73-ab41bc5a77e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Загрузка модели и эмбедов\n",
    "# --- КОНФИГУРАЦИЯ ---\n",
    "# Для 20GB VRAM: \n",
    "# Безопасно: \"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\" (Контекст ~16k)\n",
    "# Экстремально: \"unsloth/Qwen2.5-32B-Instruct-bnb-4bit\" (Контекст ~2-4k)\n",
    "MODEL_ID = \"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\" \n",
    "MAX_SEQ_LEN = 8192 \n",
    "\n",
    "# 1. Загрузка LLM на GPU\n",
    "print(f\"Загрузка LLM: {MODEL_ID}...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_ID,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Создаем пайплайн для LangChain\n",
    "text_generation_pipeline = torch.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024, # Длина ответа\n",
    "    temperature=0.1,     # Низкая температура для фактов\n",
    "    repetition_penalty=1.15,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# 2. Загрузка Эмбеддингов на CPU (!!! Важно для экономии памяти)\n",
    "print(\"Загрузка модели эмбеддингов (CPU)...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-large\",\n",
    "    model_kwargs={'device': 'cpu'}, # Вся работа идет в RAM, не VRAM\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "print(\"Система готова!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726d190-c368-48d9-bbff-1ad4d7de6a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Смешная векторная база с чанками. Может пригодиться\n",
    "def create_db(file_paths):\n",
    "    docs = []\n",
    "    for file_path in file_paths:\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        else:\n",
    "            loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "        docs.extend(loader.load())\n",
    "    \n",
    "    # Режем текст. chunk_size=800 — оптимально для захвата смысла.\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "    splits = splitter.split_documents(docs)\n",
    "    \n",
    "    # Создаем базу. \n",
    "    # Если база уже есть, удаляем старую коллекцию, чтобы не было дублей\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=splits, \n",
    "        embedding=embeddings,\n",
    "        collection_name=\"rag_collection\"\n",
    "    )\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": 3}) # k=3 фрагмента\n",
    "\n",
    "# Пример использования (создадим файл для теста)\n",
    "with open(\"tech_guide.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Qwen2.5-32B — отличная модель, но требует много памяти. Для 20GB VRAM лучше использовать версию 14B. Unsloth ускоряет инференс в 2 раза.\")\n",
    "\n",
    "retriever = create_db([\"tech_guide.txt\"])\n",
    "print(\"База знаний обновлена.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f0487-d8c3-4166-baa6-f2ac345dfe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Шаблон промпта. Qwen любит четкие инструкции \"System\" и \"User\".\n",
    "template = \"\"\"<|im_start|>system\n",
    "Ты — профессиональный аналитик. Твоя задача — отвечать на вопросы, строго основываясь на предоставленном контексте.\n",
    "Если информации в контексте нет, честно скажи \"Я не нашел информации в документах\".\n",
    "Не придумывай факты. Отвечай на русском языке.\n",
    "\n",
    "Контекст:\n",
    "{context}<|im_end|>\n",
    "<|im_start|>user\n",
    "Вопрос: {question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Сборка цепочки: Поиск -> Форматирование -> Промпт -> LLM -> Парсинг ответа\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- ТЕСТ ---\n",
    "query = \"Какую модель лучше выбрать для 20GB VRAM?\"\n",
    "print(f\"Вопрос: {query}\\n\")\n",
    "response = rag_chain.invoke(query)\n",
    "print(f\"Ответ:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a10dc-6cf9-4ebf-b40f-4383506e9248",
   "metadata": {},
   "source": [
    "Часть 2: Гайд по Промпт-инжинирингу\n",
    "Промпт — это интерфейс программирования LLM на естественном языке. Для Qwen (и других современных моделей) работает формула \"CO-STAR\" или ее упрощенные вариации.\n",
    "1. Универсальная формула промпта (CO-STAR)\n",
    "Используйте эту структуру для сложных задач (саммаризация, анализ, кодинг):\n",
    "C (Context): Введи в курс дела.\n",
    "O (Objective): Какова цель? Что нужно сделать?\n",
    "S (Style): В каком стиле писать (деловой, код, для ребенка)?\n",
    "T (Tone): Эмоциональная окраска (строгий, дружелюбный).\n",
    "A (Audience): Для кого этот ответ?\n",
    "R (Response): Формат ответа (JSON, список, Markdown).\n",
    "Пример промпта (Саммаризация):\n",
    "(Context) Я загрузил техническую документацию по серверному оборудованию.\n",
    "(Objective) Мне нужно выделить основные риски эксплуатации.\n",
    "(Style) Технический, сухой язык.\n",
    "(Response) Сделай маркированный список из 5 пунктов на русском языке.\n",
    "2. Техники для улучшения качества\n",
    "A. Zero-Shot vs Few-Shot\n",
    "Zero-Shot: Просто просишь. \"Переведи это: ...\"\n",
    "Few-Shot (Примеры): Даешь примеры \"Вход -> Выход\". Это самый мощный способ заставить модель следовать формату.\n",
    "code\n",
    "Text\n",
    "Задача: Преврати описание в JSON.\n",
    "Пример 1:\n",
    "Текст: \"Иван купил яблоко.\" -> JSON: {\"person\": \"Иван\", \"action\": \"buy\", \"object\": \"apple\"}\n",
    "\n",
    "Текст: \"Мария продала машину.\" -> JSON\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "B. Chain of Thought (CoT) — \"Думай шаг за шагом\"\n",
    "Если задача требует логики (математика, дедукция), добавьте в промпт фразу:\n",
    "\"Давай подумаем шаг за шагом\" (Let's think step by step).\n",
    "Для Qwen это работает магически. Это заставляет модель генерировать промежуточные рассуждения перед финальным ответом.\n",
    "\n",
    "\n",
    "\n",
    "C. Ролевая модель (Persona)\n",
    "Всегда задавайте роль. LLM — это актер.\n",
    "Плохо: \"Напиши код для сайта.\"\n",
    "Хорошо: \"Ты — Senior Python Backend разработчик с 10-летним стажем. Напиши безопасный код на FastAPI...\"\n",
    "\n",
    "\n",
    "\n",
    "3. Специфика Qwen (Qwen2.5)\n",
    "Qwen обучался на огромном массиве данных и хорошо понимает спец. токены.\n",
    "Язык: Если вы спрашиваете на русском, но модель срывается на английский (бывает в коде), добавьте в конце: \"Ответ должен быть строго на русском языке\" или \"Translate output to Russian\".\n",
    "Системный промпт: Qwen очень чувствителен к началу диалога. В коде (как в пайплайне выше) всегда оборачивайте инструкции в теги <|im_start|>system ... <|im_end|>.\n",
    "Запреты: Модели плохо понимают отрицания (\"НЕ делай это\"). Лучше писать позитивные инструкции (\"Вместо этого делай то\").\n",
    "Плохо: \"Не пиши длинные предложения.\"\n",
    "Хорошо: \"Пиши короткими, рублеными фразами.\"\n",
    "Шпаргалка для ваших задач\n",
    "\n",
    "\n",
    "\n",
    "Задача: RAG (Ответ по документу)\n",
    "\"Ты — эксперт по [тема документа]. Используй только представленный ниже контекст для ответа. Не используй свои внешние знания. Если контекста недостаточно, скажи об этом. Цитаты из текста приветствуются.\"\n",
    "\n",
    "\n",
    "\n",
    "Задача: Саммаризация\n",
    "\"Прочитай текст ниже. Твоя задача — создать краткую выжимку (Executive Summary). Выдели 3 главные идеи и ключевые выводы. Игнорируй вводные слова и воду. Формат: заголовок + буллет-поинты.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Задача: Файнтюн (Подготовка данных)\n",
    "\"Перепиши этот грязный текст в формат вопрос-ответ. Вопрос должен быть от лица новичка, а ответ — от лица профессионала. Соблюдай структуру JSON.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9075c4-d912-4a40-93af-113d36140dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
