{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f9dfd5-7207-4c9c-baba-c4a7759b0bf4",
   "metadata": {},
   "source": [
    "Блок 1: Теория (LoRA vs QLoRA и что тюнить)\n",
    "1. LoRA или QLoRA?\n",
    "LoRA: Вы замораживаете модель и учите только маленькие адаптеры. Требует загрузки базы в 16-bit. На 20 Гб VRAM вы сможете тюнить так только модели размером 7B.\n",
    "QLoRA: Вы загружаете базу в 4-bit (сжимаете), а адаптеры учите сверху. Это экономит огромное количество памяти.\n",
    "Ваш выбор: Только QLoRA (позволит тюнить 14B на 20Гб карте).\n",
    "2. Какие модули тюнить?\n",
    "В трансформерах (архитектура LLM) есть слои внимания (Attention) и полносвязные слои (MLP).\n",
    "Минимум: q_proj, v_proj (Query/Value в Attention). Это дает средний результат.\n",
    "Максимум (рекомендуется): Тюнить все линейные слои (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj). Unsloth делает это автоматически одной настройкой. Это дает максимальный прирост ума модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f255f8-0662-4a7d-af58-47fd543c4bed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "# Конфигурация\n",
    "max_seq_length = 2048 # Длина контекста при обучении. Чем больше, тем больше нужно VRAM.\n",
    "dtype = None # Auto detection (bfloat16 если карта новая, float16 если старая)\n",
    "load_in_4bit = True # Включаем QLoRA (обязательно для 20GB VRAM)\n",
    "\n",
    "model_name = \"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\" \n",
    "\n",
    "# 1. Загружаем базу\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# 2. Навешиваем адаптеры LoRA\n",
    "# Unsloth сам определит целевые модули (target_modules) для Qwen\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Ранг матрицы. 16 - стандарт, 64 - для сложных задач (ест больше памяти)\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16, # Обычно равно r или r*2.\n",
    "    lora_dropout = 0, # Для оптимизации памяти ставим 0\n",
    "    bias = \"none\",    # Тоже для оптимизации\n",
    "    use_gradient_checkpointing = \"unsloth\", # Критически важно для экономии VRAM!\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  \n",
    "    loftq_config = None, \n",
    ")\n",
    "\n",
    "print(\"Модель готова к обучению с QLoRA!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b26e3b6-acb8-46db-962a-43348208cd0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Пример данных для дообучения (RAG-стиль или саммаризация)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Вам нужно подготовить список словарей\u001b[39;00m\n\u001b[0;32m      5\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     {\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mСуммируй следующий текст.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# ... добавьте сюда свои 100-1000 примеров\u001b[39;00m\n\u001b[0;32m     17\u001b[0m ]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Пример данных для дообучения (RAG-стиль или саммаризация)\n",
    "# Вам нужно подготовить список словарей\n",
    "raw_data = [\n",
    "    {\n",
    "        \"instruction\": \"Суммируй следующий текст.\",\n",
    "        \"input\": \"Qwen2.5 - это новая модель, которая показывает отличные результаты в кодинге и математике...\",\n",
    "        \"output\": \"Qwen2.5 - мощная модель для кода и математики.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Ответь на вопрос по контексту.\",\n",
    "        \"input\": \"Контекст: Компания Unsloth ускоряет обучение LLM.\\nВопрос: Что делает Unsloth?\",\n",
    "        \"output\": \"Компания Unsloth занимается ускорением обучения больших языковых моделей.\"\n",
    "    }\n",
    "    # ... добавьте сюда свои 100-1000 примеров\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_list(raw_data)\n",
    "\n",
    "# Шаблон промпта (должен совпадать с тем, как модель училась, или быть стандартным Alpaca)\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Нужно добавить токен конца генерации, иначе модель не заткнется\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Формируем полный текст\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "# Применяем форматирование\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8c7e50-ebaa-4c9f-91fc-90fe8f577d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
