. Как работает пайплайн (объяснение)
Инициализация (__init__):
Мы загружаем LLM через unsloth. Это экономит до 30% памяти по сравнению со стандартным transformers.
Мы выставляем load_in_4bit=True. Это сжимает веса.
Эмбеддинг-модель (multilingual-e5-large) принудительно отправляется на cpu. Она весит около 1-2 Гб. Если загрузить её в VRAM рядом с 32B моделью, у вас случится OOM (Out Of Memory). CPU справится с векторизацией за пару секунд, это не критично.
RAG (Поиск + Ответ):
Документ разбивается на чанки.
RetrievalQA ищет 3 самых похожих куска.
Эти куски вставляются в промпт. Важно: Если вы используете Qwen 32B, у вас очень мало места для контекста. Если 3 куска займут больше 1-2k токенов, модель может упасть. Уменьшите k=3 до k=2 в коде (search_kwargs), если будут ошибки памяти.
Суммаризация:
Если документ короткий -> stuff. Модель видит его целиком.
Если длинный -> map_reduce. Модель видит его по частям. Это занимает больше времени, но не требует много памяти за один раз. Идеально для 20GB VRAM.
3. Тонкая настройка под Qwen 32B на 20GB
Если вы твердо решили использовать 32B (она действительно умнее):
В коде найдите max_seq_length. Установите 4096 или даже 2048.
В методе chat_rag уменьшите количество кусков: k=2 или k=1.
Альтернатива: Gemma-2-27B
Это модель от Google. Она очень хороша в рассуждениях (Reasoning).
Вес: ~16.5 Гб.
Запас: ~3.5 Гб.
Это дает вам возможность поставить max_seq_length = 6000 или 8000, что гораздо комфортнее для работы с документами, чем Qwen 32B впритык.
В коде просто поменяйте model_name на unsloth/gemma-2-27b-it-bnb-4bit.
